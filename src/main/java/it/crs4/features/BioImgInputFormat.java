package it.crs4.features;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.LocatedFileStatus;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.BlockLocation;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.JobContext;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.io.NullWritable;

import com.google.common.base.Stopwatch;

import org.apache.avro.generic.IndexedRecord;

import loci.formats.ImageReader;
import loci.formats.FormatException;


/**
 * Similar to FileInputFormat, but with series-based splits
 */
public class BioImgInputFormat
    extends FileInputFormat<NullWritable, IndexedRecord> {

  private static final Log LOG = LogFactory.getLog(BioImgInputFormat.class);

  public static final String NUM_INPUT_FILES =
    "mapreduce.input.fileinputformat.numinputfiles";

  @Override
  protected boolean isSplitable(JobContext context, Path filename) {
    return false;  // for now; see getSplits()
  }

  @Override
  public List<InputSplit> getSplits(JobContext job) throws IOException {
    Stopwatch sw = new Stopwatch().start();
    List<InputSplit> splits = new ArrayList<InputSplit>();
    List<FileStatus> files = listStatus(job);
    for (FileStatus file: files) {
      Path path = file.getPath();
      // Since it comes from a FileStatus, it's an absolute path
      String absPathName = path.toString();
      long length = file.getLen();
      if (length == 0) {
        throw new RuntimeException(absPathName + " is empty");
      }
      BlockLocation[] blkLocations;
      if (file instanceof LocatedFileStatus) {
        blkLocations = ((LocatedFileStatus) file).getBlockLocations();
      } else {
        FileSystem fs = path.getFileSystem(job.getConfiguration());
        blkLocations = fs.getFileBlockLocations(file, 0, length);
      }
      if (isSplitable(job, path)) {
        // disabled until we know if and how we can map a
        // Bio-Formats series to an HDFS block.
        throw new UnsupportedOperationException("can't split file");
      } else {
        // one split per series
        long len = 1;
        ImageReader reader = new ImageReader();
        try {
          reader.setId(absPathName);
        } catch (FormatException e) {
          throw new RuntimeException("FormatException: " + e.getMessage());
        }
        int nSeries = reader.getSeriesCount();
        reader.close();
        for (long i = 0; i < nSeries; i++) {
          // For now we just hack the default FileSplit
          splits.add(makeSplit(path, i, len, blkLocations[0].getHosts(),
                               blkLocations[0].getCachedHosts()));
        }
      }
    }
    // Save the number of input files for metrics/loadgen
    // Copied verbatim from FileInputFormat
    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());
    sw.stop();
    if (LOG.isDebugEnabled()) {
      LOG.debug("Total # of splits generated by getSplits: " + splits.size()
          + ", TimeTaken: " + sw.elapsedMillis());
    }
    return splits;
  }

  @Override
  public RecordReader<NullWritable, IndexedRecord> createRecordReader(
      InputSplit split, TaskAttemptContext context) {
    return new BioImgRecordReader();
  }

}
